{"cells":[{"source":["# Coursera_Capstone\n","<p> This project is part of the IBM Data Science Professional Certification provided by Coursera and its main objective is to apply data science concepts to analyze a real-world scenario. </p>\n","\n","# 1.Introduction\n","<p>\n","One of the current concerns of the Seattle Department of Transportation is to find solutions that can minimize the number of car accidents, as well as fatalities, injuries and damages due to traffic accidents, however, before solving a problem it is extremely important to study it and understand it in full. In this context, all relevant information of occurrences is recorded and maintained by the department for the access of all researchers.\n","\n","This project intends to use these records to analyze all the collisions recorded from 2003 to October 2020 in order to build a Machine Learning Model that allows the severity of an accident to be classified by its characteristics.\n","</p>\n","\n","<p> To achieve the objective, the project applies data science concepts such as Machine Learning and Data Visualization under the CRISP-DM methodology, which is a model focused on communication, flexibility and iteration at each stage of the project to keep it on track right. The project was designed and structured based on 6 stages of the CRISP-DM methodology namely,Business understanding, Data understanding, Data preparation, Modeling, Evaluation and Implementation.</p>"],"cell_type":"markdown","metadata":{"collapsed":true}},{"metadata":{},"cell_type":"markdown","source":["# 2.Data Understanding\n","\n","<p>\n","The data set used in this project is available in a comma-separated values (CSV) file format and has been downloaded from <a href=\"https://data-seattlecitygis.opendata.arcgis.com/datasets/5b5c745e0f1f48e7a53acec63a0022ab_0?geometry=-122.326%2C47.592%2C-122.318%2C47.594\" target=\"_blank\">Seattle Open GeoData Portal</a> and includes all types of collisions since 2004 to Present. The dataset metadata was found at <a href=\"https://www.seattle.gov/Documents/Departments/SDOT/GIS/Collisions_OD.pdf\" target=\"_blank\"> Department of Transportation Seattle</a>.\n","The dataset contains 221738 rows/registers and 40 columns/fields. The dataset contains columns with 3 diferent type of values, float64, object and Int64.\n","</p>\n"]},{"metadata":{},"cell_type":"markdown","source":["# 3.Data Preparation and Cleaning"]},{"metadata":{},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","%matplotlib inline\n","\n","#!conda install seaborn -y\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["#### Loading Data"]},{"source":["#!wget -O Collisions.csv \"https://opendata.arcgis.com/datasets/5b5c745e0f1f48e7a53acec63a0022ab_0.csv\""],"cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["df_raw = pd.read_csv('Collisions.csv')\n","df_raw.info()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["<b>About data...</b><br>\n","1 - There is 221738 records in 40 columns and not all of columns have the same number of records.<br>\n","2 - The data contain categorical and continuos values distribuided in float64(5), int64(12), object(23)."]},{"metadata":{},"cell_type":"markdown","source":["<b> Balancing data </b> <br>\n","During Data Understanting some other projects were studied and was possible to understand wich kind of variable can be useful in this kind of analysis and that is good  before start balancing the data decide first  columns that contain information that apparently can be useful for the analysis goals.\n","\n","Observing the data is notable tha there are columns that represent CODES that are irrelevant to our goals such: <code>OBJECTID</code>, <code>INCKEY</code>, <code>INTKEY</code>, <code>SDOT_COLCODE</code>, <code>SEGLANEKEY</code>, <code>INATTENTIONIND</code> and others keys <br>"]},{"metadata":{},"cell_type":"code","source":["df = df_raw[['X','Y','SEVERITYCODE','COLLISIONTYPE','PERSONCOUNT','PEDCOUNT','PEDCYLCOUNT',\n","            'VEHCOUNT','INJURIES','SERIOUSINJURIES','FATALITIES','WEATHER','ROADCOND','SPEEDING','JUNCTIONTYPE','UNDERINFL','LIGHTCOND',                       'HITPARKEDCAR','PEDROWNOTGRNT','INATTENTIONIND','INCDATE','SEVERITYDESC']]"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["missing_df = df.isnull().sum(axis=0).reset_index()\n","missing_df= missing_df[missing_df[0]>0]\n","missing_df"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["#### Handling with missing values <br>\n","\n","During data understanding was noticed that:<br>\n","1 - The values of column <code>SPEEDING</code> behave as boolean where value <code>Y</code> represents <code>TRUE</code> and  <code>(empty)</code> represents <code>FALSE</code>.<br>\n","2 - The values of <code>SEVERITYCODE</code>  can be <code>1,2,3,2b</code> and value <code>0</code> means unknown<br>\n","3 - All columns with values <code>Y</code> and <code>N</code> have boolean behave<br>\n","4 - Columns use values <code>Unknown</code> and <code>Other</code> also to express missing information.<br>\n","\n","<br>\n","All this finding was applied before start droping data."]},{"metadata":{},"cell_type":"code","source":["df[\"SEVERITYCODE\"].replace('0', np.nan, inplace=True)\n","df[\"SEVERITYCODE\"].replace('2b', 4, inplace=True)\n","df[\"SPEEDING\"].replace(np.nan,0 , inplace=True)\n","df[\"PEDROWNOTGRNT\"].replace(np.nan,0 , inplace=True)\n","df[\"INATTENTIONIND\"].replace(np.nan,0 , inplace=True)\n","df.replace({\"Y\": 1, '1':1} , inplace=True)\n","df.replace({\"N\": 0, '0':0} , inplace=True)\n","df.replace(\"Unknown\", np.nan , inplace=True)\n","df.replace(\"Other\", np.nan , inplace=True)\n","\n","\n","df['MONTH']=pd.to_datetime(df['INCDATE']).dt.month\n","df['DAYOFWEEK']=pd.to_datetime(df['INCDATE']).dt.dayofweek\n","df.drop(['INCDTTM'],axis=1, inplace=True)\n","\n","\n","#To make easy to use math functions\n","df[\"SEVERITYCODE\"] = df[\"SEVERITYCODE\"].apply(pd.to_numeric, downcast='integer', errors='coerce')\n"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["missing_df = df.isnull().sum(axis=0).reset_index()\n","missing_df= missing_df[missing_df[0]>0]\n","missing_df"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["print('Before', df.shape)\n","df.dropna(axis=0, inplace=True)\n","print('After', df.shape)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":["df_clean = df\n","df_clean.head(10)"],"execution_count":null,"outputs":[]},{"source":["Even with cleaning is possible to see mixed type that represent Categorical and Continuous data.<br>\n","We will split our data exploration folowing this scenario.\n","\n","But first lets take a look on our Target <code>SEVERITY</code>"],"cell_type":"markdown","metadata":{}},{"source":["### Data visualization and pre-processing "],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ax=plt.subplots(figsize=(11,8))\n","sns.set(style='darkgrid')\n","sns.countplot('SEVERITYDESC',data=df_clean,ax=ax[1],order=df_clean['SEVERITYDESC'].value_counts().index)\n","ax[1].set_title('Count of Severity')\n","plt.show()"]},{"source":["#### Continuos Features"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["continuous_features = [col for col in df_clean.columns if df_clean[col].dtype=='float64' or df_clean[col].dtype=='int64']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","plt.figure(figsize=(16,13))\n","matrix = np.triu(df_clean[continuous_features].corr())\n","sns.heatmap(df_clean[continuous_features].corr(), annot=True, mask=matrix)\n"]},{"source":["As can be seen, the most correlated variable to the target is <code>INJURIES</code>, followed by <code>SERIOUSINJURIES</code> and <code>PEDCOUNT</code>.\n","\n","Is important to analysi each of this variables separately to understand the importance to accident classification. The better way to do that is using visualization resources.\n"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.set_theme(); np.random.seed(0)\n","sns.jointplot(x=df_clean.X.values,y=df_clean.Y.values,height=10, data=df_clean, hue=\"SEVERITYDESC\")\n","sns.color_palette(\"dark\", 10)\n","plt.ylabel('Latitude', fontsize=12)\n","plt.xlabel('Longitude', fontsize=12)\n","plt.show()"]},{"source":["Looking at the geodistribution map, we can see that the accident's Severity distribution has almost the same proportions in terms of occurrence. Therefore, we can assume that the site has a weak impact to determine the severity of the accident."],"cell_type":"markdown","metadata":{}},{"source":["Continuing with other variables"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["continuous_features.remove('X')\n","continuous_features.remove('Y')\n","continuous_features.remove('SEVERITYCODE')\n","\n","\n","fig, axs = plt.subplots(ncols=3, nrows=3, figsize=(20, 18))\n","plt.subplots_adjust(hspace=1.0,wspace = 0.5)\n","for i, feature in enumerate(continuous_features, 1):    \n","    plt.subplot(3, 3, i)\n","    sns.set(style='darkgrid')\n","    ax = sns.barplot(x = 'SEVERITYDESC', y = feature, data = df_clean)\n","    plt.xticks(rotation=30, horizontalalignment='right',fontweight='light') \n","    plt.title('SEVERITY IN {}'.format(feature), size=14, y=1.05)\n","    ax.set_xlabel('')\n","fig.suptitle('SEVERITY VS CONTINUOUS VARIABLES',y=1.02, fontsize=16)\n","plt.show()\n"]},{"source":["There a very insteresting connections between the potencial features and the target. Analysing each of the graphs we can se that:<br>\n","<ul>\n","    <li><p>INJURIES - Accident involving a high number of <code>INJURIES</code>  tends to be classified as \n","    <b>Serious Injury Colision</b> and with thw lower as  <b>Property Damage Only Collision</b>. This is a good feature to the model</p>\n","    </li><br>\n","    <li>SERIOUSINJURIES - Accidents envloving serious injuries tends to be classified as <b>Serious Injury Colision</b> or \n","        <b>Fatality Collision</b>. \n","    </li><br>\n","    <li>SPEEDING or UNDERINFL or PEDCOUNT - Accidents caused by high speed or driving under the influence of drugs or even accidents involving a   considerable number of pedestrians tend to be classified as <b> Collision Fatality </b>. The classification tends to be less severe when these variables reduces the value.\n","    </li><br>\n","    <li>\n","        PEDCYLCOUNT - Accident envloving high number of pedestrians with bicycle tend to be classified as <b> Serious Injury Collision</b> it makes sense because whenever a car collides with a motorcyclist the cyclist is more likely to suffer serious injuries and can sometimes be fatal and when it happens is classificaed as <b> Fatality Collision</b> \n","    </li><br>\n","    <li> FATALITIES - Accidents with <b>Fatalies<b> regardless the quantity is classificated as <b>Fatality Collision<b/>\n","    </li><br>\n","    <li> PERSONCOUNT - Accident envloving high number of person tend to be classified as <b> Fatalies Collision</b> it because in most of these casesthe probability of at least one Fatality is higer.\n","    </li><br>\n","    <li> <p>VHECOUNT - Accident envloving high number of Vehicles tend to be classified as <b> Property Damage Only Collision</b>.that probably\n","it happens because other cars around the crash site suffer minor damage caused by the collision inertia, and there have been no human victims because the cars are empty and parked. \n","But it is not so linear, if combined with other variables such as FATALITIES, it is automatically classified as <b> Fatality Collision</b>.</p>\n","    </li>   \n","</ul>\n"],"cell_type":"markdown","metadata":{}},{"source":["### Categorical Features"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["categorical_features = [col for col in df_clean.columns if df_clean[col].dtype=='object']\n","categorical_features.remove('SEVERITYDESC')\n","categorical_features"]},{"source":["Lets explore the distirbuiton of each variable in accident severity so far."],"cell_type":"markdown","metadata":{}},{"source":["fig, axs = plt.subplots(ncols=1, nrows=5, figsize=(12, 24))\n","plt.subplots_adjust(hspace=1.3,wspace = 0.5)\n","for i, feature in enumerate(categorical_features, 1):    \n","    plt.subplot(5, 1, i)\n","    sns.set(style='darkgrid')\n","    ax = sns.countplot(x=feature, hue = 'SEVERITYDESC', data = df_clean)\n","    plt.xticks(rotation=30, horizontalalignment='right',fontweight='light') \n","    plt.title('SEVERITY IN {}'.format(feature), size=14, y=1.05)\n","    plt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=1)\n","    ax.set_xlabel('')\n","fig.suptitle('SEVERITY VS CATEGORICAL VARIABLES',y=.95, fontsize=16)\n","plt.show()"],"cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":["As we can see the categorical variables (just a few values ​​of each variable) only have an impact on the classification of accidents <b>Property Damage Collision</b> and <b>Injury Collision</b>. "],"cell_type":"markdown","metadata":{}},{"source":[],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feautures = ['SEVERITYCODE']+continuous_features+categorical_features\n","\n","df_cleaned = pd.get_dummies(df_clean[feautures], columns=categorical_features, drop_first=True)\n","\n","df_corr = pd.DataFrame(df_cleaned.corr()['SEVERITYCODE'].sort_values(ascending=False)).reset_index()\n","df_corr.columns = ['features','correlation']\n","df_corr[1:]"]},{"source":["Converting Categorical features in continuous create a lot of variables to the data set, but as we saw when we was observing categorical variables not all of categories values have important impact to classificate accident severity, therefore, all the variables with -0.009 < correlaction < 0.009 will be droped because it means that there is no historical data enought to help us classifying an accident based in this value."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_corr = df_corr.loc[(df_corr['correlation'] > 0.009) | (df_corr['correlation'] <-0.009)]\n","df_corr[1:]\n","feautures = df_corr['features'].values[1:].tolist()\n","feautures"]},{"source":["Setting the Features"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = df_cleaned[feautures]\n","X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))\n","X[:5]"]},{"source":["Setting the Target"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y = df_cleaned['SEVERITYCODE']\n","y[:5]"]},{"source":["# 4. Modeling and Evaluation\n","\n","<p>\n","After processing the dataset and find the features, now is time to build the model to predict the accident severity based on data historical, to achieve this will be used the train split test approach and also be used the KNN, Decision Tree Classification and Logistic Regression as   classification models. \n","\n","<code> accuracy.score</code> and <code>f1score.score</code> to evaluate model accuracy.\n","</p>"],"cell_type":"markdown","metadata":{}},{"source":["###  Train Test Split"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n","print ('Train set:', X_train.shape,  y_train.shape)\n","print ('Test set:', X_test.shape,  y_test.shape)"]},{"source":["### Logistic Regression"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","\n","LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\n","\n","lr_pred = LR.predict(X_test)\n","lr_acc = round(accuracy_score(y_test, lr_pred)*100,2)\n","lr_f1 = round(f1_score(y_test, lr_pred, average='weighted')*100,2)\n"," \n","print(\"Test Accurancy: %.1f%%\"% (lr_acc))\n","print(\"Test F1 Accurancy: %.1f%%\"% (lr_f1))\n"]},{"source":["### Decision Tree Classification"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","dtc = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\n","dtc.fit(X_train,y_train)\n","\n","dtc_pred = dtc.predict(X_test)\n","dtc_acc = round(accuracy_score(y_test, dtc_pred)*100,2)\n","dtc_f1 = round(f1_score(y_test, dtc_pred, average='weighted')*100,2)\n","\n","print(\"Decision Tree Classification Accurracy %.1f%%\"% (dtc_acc))\n","print(\"Decision Tree Classification f1 Accurracy %.1f%%\"% (dtc_f1))\n"]},{"source":["### K-Nearest Neighbors - KNN"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","Ks = 10\n","mean_acc = np.zeros((Ks-1))\n","std_acc = np.zeros((Ks-1))\n","\n","for n in range(1,Ks):\n","    \n","    #Train Model and Predict  \n","    neigh = KNeighborsClassifier(n_neighbors = n, n_jobs=-1).fit(X_train,y_train)\n","    \n","    knn_pred=neigh.predict(X_test)\n","    \n","    mean_acc[n-1] = accuracy_score(y_test, knn_pred)\n","    \n","    std_acc[n-1]=np.std(knn_pred==y_test)/np.sqrt(knn_pred.shape[0])\n","\n","mean_acc"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(range(1,Ks),mean_acc,'g')\n","plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\n","plt.legend(('Accuracy ', '+/- 3xstd'))\n","plt.ylabel('Accuracy ')\n","plt.xlabel('Number of Nabors (K)')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["knn_acc = round(mean_acc.max()*100,1)\n","k = mean_acc.argmax()+1\n","print( \"KNN test accuracy:  %.1f%%\"% (knn_acc))\n","print(\"K = \" , k )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Machine Learning algorithm scores without weather related conditions\")\n","models = pd.DataFrame({\n","    'Model': ['Logistic Regression C', 'Decision Tree C', 'KNN'],\n","    'Score': [lr_acc, dtc_acc, knn_acc]})\n","models.sort_values(by='Score', ascending=False)"]},{"source":["# 5.Conclusion\n","<p>\n","The goal of the project was achieved, the model was build using differents \n","</p>\n","\n","<p>Is very important to select the target after test all the variables with diferents approaches. If only variables with high correlaction coeficient with the target was considered in this project we could risk to exclude some other usefull features to our model.</p>\n","\n","<p></p> Visualization was a better way to express the importance of each variables used in model."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"name":"Python 3.8.2 32-bit","display_name":"Python 3.8.2 32-bit","metadata":{"interpreter":{"hash":"6fa8c4a0213b3e8e46e64ca221d4ef2f7254b1e53b83d6209b624a99d7aa7db4"}}},"language_info":{"name":"python","version":"3.8.2-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}