{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Introduction\n",
    "<p>\n",
    "One of the current concerns of the Seattle Department of Transportation (SDT) is to find solutions that can minimize the number of car accidents as well as fatalities, injuries and damages due to traffic accidents, in this context, all relevant information of accidents occurrences is recorded and maintained by the department for the access of all researchers. This data is necessary for identifying the locations and causes of crashes, for planning and implementing countermeasures, for operational management and control, and for evaluating highway safety programs and improvements.\n",
    "</p>\n",
    "\n",
    "\n",
    "</p>\n",
    "The problem is that accidents have a different severity and implementation of countermeasures must be able to prioritize the implementation of solutions based on this. SDT classifies each of the registered collision with one of four categories, namely, Property Damage Only Collision, Injury Collision, Serious Injury Colision and Fatality Collision. This project intends to use these records to analyze all the collisions registered since 2004, in order to build a Machine Learning Model that will make it possible to classify the severity of a new collisions based on its characteristics.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Data understanding\n",
    "\n",
    "<p> The data set used in this project is available in a comma-separated values ​​(CSV) file format and was downloaded from the <a href=\"https://data-seattlecitygis.opendata.arcgis.com/datasets/5b5c745e0f1f48e7a53acec63a0022ab_0?geometry=-122.326%2C47.592%2C-122.318%2C47.594\" target=\"_blank\">Seattle Open GeoData Portal</a>  and includes all types of collisions from 2004 to the present. The data set contains 221738 rows / records and 40 columns / fields. The data set contains columns with 3 different types of values, float64, object and Int64. </p>\n",
    "<p> The data set metadata was found at the <a href=\"https://www.seattle.gov/Documents/Departments/SDOT/GIS/Collisions_OD.pdf\" target=\"_blank\"> Department of Transportation Seattle</a>. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "!conda install seaborn -y\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -O Collisions.csv \"https://opendata.arcgis.com/datasets/5b5c745e0f1f48e7a53acec63a0022ab_0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('Collisions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "<p> We tried to keep as many resources as possible after cleaning. In this sense, the following steps were performed: </p>\n",
    "\n",
    "<p> a) Data set is part of an information system of the Seattle government and relates to other tables, with this, some columns representing secondary keys of these supplies were all discarded as well as the primary key and other columns of references. </ p>\n",
    "\n",
    "<p> b) Columns with Boolean behavior values ​​where Y corresponds to TRUE and N or null value field (NAN) corresponds to FALSE were standardized for numeric values ​​0 and 1, where 1 = TRUE and 0 and NAN = FALSE. </p>\n",
    "\n",
    "<p> c) Day of the week and Month was extracted from the column containing the date of occurrence and the original column was discarded. </p>\n",
    "\n",
    "<p> d) The target variable appears represented by two columns, SEVERITYCODE which has the severity code with the values ​​(1,2,3,2b, 0) has been changed to the type int64 with the values ​​(1,2, 3, 4, None) the other column is SEVERITYDESC which includes the description and severity was maintained in the dataset to give more emphasis on the interpretation of graphs. </p>\n",
    "\n",
    "<p> e) Some columns use the value \"unknown\" and \"other\" to represent missing data, so these values ​​have been replaced by NAN. </p>\n",
    "\n",
    "<p> f) Columns containing location or address information have been discarded and are fundamental to the columns of geographic coordinates. </p>\n",
    "\n",
    "<p> g) All records containing at least one NAN value have been discarded from the DataFrame. </p>\n",
    "\n",
    "<p> With the data cleaning, 221738 records generated 148171 (68%) and the 40 columns were transformed into 23 features (58%). </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw[['X','Y','SEVERITYCODE','COLLISIONTYPE','PERSONCOUNT','PEDCOUNT','PEDCYLCOUNT',\n",
    "            'VEHCOUNT','INJURIES','SERIOUSINJURIES','FATALITIES','WEATHER','ROADCOND','SPEEDING','JUNCTIONTYPE','UNDERINFL','LIGHTCOND','HITPARKEDCAR','PEDROWNOTGRNT','INATTENTIONIND','INCDATE','SEVERITYDESC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = df.isnull().sum(axis=0).reset_index()\n",
    "missing_df= missing_df[missing_df[0]>0]\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SEVERITYCODE\"].replace('0', np.nan, inplace=True)\n",
    "df[\"SEVERITYCODE\"].replace('2b', 4, inplace=True)\n",
    "df[\"SEVERITYCODE\"] = df[\"SEVERITYCODE\"].apply(pd.to_numeric, downcast='integer', errors='coerce')\n",
    "df[\"SPEEDING\"].replace(np.nan,0 , inplace=True)\n",
    "df[\"PEDROWNOTGRNT\"].replace(np.nan,0 , inplace=True)\n",
    "df[\"INATTENTIONIND\"].replace(np.nan,0 , inplace=True)\n",
    "df.replace({\"Y\": 1, '1':1} , inplace=True)\n",
    "df.replace({\"N\": 0, '0':0} , inplace=True)\n",
    "df.replace(\"Unknown\", np.nan , inplace=True)\n",
    "df.replace(\"Other\", np.nan , inplace=True)\n",
    "\n",
    "\n",
    "df['MONTH']=pd.to_datetime(df['INCDATE']).dt.month\n",
    "df['DAYOFWEEK']=pd.to_datetime(df['INCDATE']).dt.dayofweek\n",
    "df.drop(['INCDATE'],axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before', df.shape)\n",
    "df.dropna(axis=0, inplace=True)\n",
    "print('After', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df\n",
    "df_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanatory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribuiton of Severity by Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=plt.subplots(figsize=(11,8))\n",
    "sns.set(style='darkgrid')\n",
    "sns.countplot('SEVERITYDESC',data=df_clean,ax=ax[1],order=df_clean['SEVERITYDESC'].value_counts().index)\n",
    "ax[1].set_title('Distribuition of Severity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "The feature were splited in two groups Categorical Features and Continuos Features\n",
    "\n",
    "#### Continuos Features\n",
    "Continuous features Include all independent variables of type float64 and int64. Here, the cumulative value of the features was analyzed according to the severity classes of the accidents in order to identify the trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [col for col in df_clean.columns if df_clean[col].dtype=='float64' or df_clean[col].dtype=='int64']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribuition of Severidty by Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(); np.random.seed(0)\n",
    "sns.jointplot(x=df_clean.X.values,y=df_clean.Y.values,height=10, data=df_clean, hue=\"SEVERITYDESC\")\n",
    "sns.color_palette(\"dark\", 10)\n",
    "plt.ylabel('Latitude', fontsize=12)\n",
    "plt.xlabel('Longitude', fontsize=12)\n",
    "plt.show()\n",
    "#plt.savefig('severity_geo_distribuition.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the geodistribution map, we can see that the accident's Severity distribution has almost the same proportions in terms of occurrence. Therefore, we can assume that the site has a weak impact to determine the severity of the accident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribuition of Severidty by continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features.remove('X')\n",
    "continuous_features.remove('Y')\n",
    "continuous_features.remove('SEVERITYCODE')\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(ncols=5, nrows=3, figsize=(30,25))\n",
    "plt.subplots_adjust(hspace=.9,wspace = 0.5)\n",
    "for i, feature in enumerate(continuous_features, 1):    \n",
    "    plt.subplot(3, 5, i)\n",
    "    sns.set(style='darkgrid')\n",
    "    ax = sns.barplot(x = 'SEVERITYDESC', y = feature, data = df_clean)\n",
    "    plt.xticks(rotation=30, horizontalalignment='right',fontweight='light') \n",
    "    plt.title('SEVERITY IN {}'.format(feature), size=14, y=1.05)\n",
    "    ax.set_xlabel('')\n",
    "fig.suptitle('SEVERITY VS CONTINUOUS VARIABLES',y=.02)\n",
    "plt.show()\n",
    "#plt.savefig('severity_distribuition_continuous.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Looking at Figure above, it is clear that each feature influences the severity of the accident in its own way, in the SEVERITY IN FATALITIES relationship, for example, it is clear that any accident involving fatality tends to be classified as Fatality Collision, in the SEVERITY IN HIPARKEDCAR relationship, If accidents involving a high number of parked cars tend to be classified with <i>Only Property Damage Collision</i> and if the number of cars involved is relatively low then it tends to be classified as a Serious Collision or Collision with Serious Injury.</p>\n",
    "<p>\n",
    "In relationships like SEVERITY IN MONTH and SEVERITY IN DAYOFWEEK it is very difficult to classify the accident because the distribution of severity is almost equal.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [col for col in df_clean.columns if df_clean[col].dtype=='object']\n",
    "categorical_features.remove('SEVERITYDESC')\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore the distirbuiton of each variable in accident severity so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=3, figsize=(30, 24))\n",
    "plt.subplots_adjust(hspace=1,wspace = 0.5)\n",
    "for i, feature in enumerate(categorical_features, 1):    \n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.set(style='darkgrid')\n",
    "    ax = sns.countplot(x=feature, hue = 'SEVERITYDESC', data = df_clean)\n",
    "    plt.xticks(rotation=30, horizontalalignment='right',fontweight='light') \n",
    "    plt.title('SEVERITY IN {}'.format(feature), size=14, y=1.05)\n",
    "    plt.legend(bbox_to_anchor=(1.0, 0.7),borderaxespad=0.5)\n",
    "    ax.set_xlabel('')\n",
    "fig.suptitle('SEVERITY VS CATEGORICAL VARIABLES',y=.95, fontsize=16)\n",
    "#plt.savefig('severity_distribuition_categorical.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The figure above explores the relationship between the severity of the accident and the categorical features, in which we can see that some values of the variables do not contain enough information to classify the severity of the accident. These values will be discarded from the model that is intended to be created because it does not show any tendency towards the target</p>\n",
    "\n",
    "<p>After identifying the relevant numerical as well as categorical variables, a pool of categorical and continuous resources will be made and then the values of categorical variables will be transformed into dummy variables.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feautures = ['SEVERITYCODE']+continuous_features+categorical_features\n",
    "\n",
    "df_cleaned = pd.get_dummies(df_clean[feautures], columns=categorical_features, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Categorical features in continuous create a lot of variables to the data set, but as we saw when we was observing categorical variables not all of categories values have important impact to classificate accident severity, therefore, all the variables with -0.03 < correlaction < 0.03 will be droped because it means that there is no historical data enought to help us classifying an accident based in this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = pd.DataFrame(df_cleaned.corr()['SEVERITYCODE'].sort_values(ascending=False)).reset_index()\n",
    "df_corr.columns = ['features','correlation']\n",
    "\n",
    "df_corr = df_corr.loc[(df_corr['correlation'] > 0.03) | (df_corr['correlation'] <-0.03)]\n",
    "\n",
    "a = df_corr[1:]\n",
    "ax = a.plot(kind='barh', figsize=(14,18))\n",
    "ax.set_yticklabels(a[\"features\"], size=14)\n",
    "plt.title('Severity Correlations with (-0.03 > coef > 0.03)', fontsize=18)\n",
    "plt.show()\n",
    "#plt.savefig('severity_correlactions.png')\n",
    "\n",
    "feautures = df_corr['features'].values[1:].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cleaned[feautures]\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_cleaned['SEVERITYCODE']\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling and Evaluation\n",
    "\n",
    "<p>\n",
    "After processing the dataset and find the features, now is time to build the model to predict the accident severity based on data historical, to achieve this will be used the train split test approach and also be used the KNN, Decision Tree Classification and Logistic Regression as   classification models. \n",
    "\n",
    "<code> accuracy.score</code> and <code>f1score.score</code> to evaluate model accuracy.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "svm_ = svm.SVC(kernel='rbf')\n",
    "svm_.fit(X_train, y_train) \n",
    "svm_pred = svm_.predict(X_test)\n",
    "\n",
    "svm_acc = accuracy_score(y_test, svm_pred)\n",
    "svm_f1 = f1_score(y_test, svm_pred, average='weighted')\n",
    " \n",
    "print(\"Test Score Accurancy: %.1f%%\"% (svm_acc*100))\n",
    "print(\"Test F1 Accurancy: %.1f%%\"% (svm_f1*100))\n",
    "print('\\n',classification_report(y_test, svm_pred))\n",
    "\n",
    "list_result = []\n",
    "list_result.append(['Support Vector Machine',svm_acc,svm_f1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\n",
    "dtc.fit(X_train,y_train)\n",
    "\n",
    "dtc_pred = dtc.predict(X_test)\n",
    "dtc_acc = accuracy_score(y_test, dtc_pred)\n",
    "dtc_f1 = f1_score(y_test, dtc_pred, average='weighted')\n",
    "\n",
    "print(\"Decision Tree Classification Score Accurracy %.1f%%\"% (dtc_acc*100))\n",
    "print(\"Decision Tree Classification f1 Accurracy %.1f%%\"% (dtc_f1*100))\n",
    "print ('\\n',classification_report(y_test, dtc_pred))\n",
    "list_result.append(['Decision Tree Classification',dtc_acc,dtc_f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Train Model and Predict  \n",
    "neigh = KNeighborsClassifier(n_neighbors = 1, n_jobs=-1)\n",
    "neigh.fit(X_train,y_train)\n",
    "    \n",
    "knn_pred = neigh.predict(X_test)\n",
    "knn_acc = accuracy_score(y_test, knn_pred)\n",
    "knn_f1 = f1_score(y_test, knn_pred, average='weighted')\n",
    "print( \"KNN test accuracy:  %.1f%%\"% (knn_acc))\n",
    "print( \"KNN f1 score accuracy:  %.1f%%\"% (knn_f1*100))\n",
    "print ('\\n',classification_report(y_test, knn_pred*100))\n",
    "list_result.append(['K-Nearest Neighbors ',knn_acc,knn_f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {\"Algoritm\":[],\"F1 Score\":[],\"Classification Accuracy\":[]};\n",
    "for resul in list_result:\n",
    "    my_dict[\"Algoritm\"].append(resul[0])\n",
    "    my_dict[\"F1 Score\"].append(resul[1])\n",
    "    my_dict[\"Classification Accuracy\"].append(resul[2])\n",
    "\n",
    "results = pd.DataFrame(my_dict)\n",
    "results\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# 5. Discussion\n",
    "\n",
    "<p>\n",
    "The data set used in the reference project contained 221,738 records corresponding to collision occurrences so far, however, after data cleaning only 148,171 records were used as a sample, this is mainly due to the lack of information in some fields. 32% of records have at least 1 missing data.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Columns with missing data is mostly represented by categorical variables, which during our analysis were noted there was insufficiency of data to analyze some values of the categorical variables and consequently they ended up being dropped because the accident cannot be classified through the variables. </p>\n",
    "<p>\n",
    "It cannot be categorically stated that the missing data are the dropped values, but it is undeniable that a data set without missing data enables the construction of richer models and it is important to standardize the data collection process.\n",
    "</p>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "<p>Purpose of this project was to analyze Seattle collisions data and build a machine learning model in order to predict the classification of accident severity by its characteristics. By splitting the variables in categorical and continuous we identified and select as features the independent variables that have significative impact in accident classification. Several classification models were built and tested obtaining an average result of 100% accuracy using different accuracy metrics.</p>\n",
    "\n",
    "<p>The implementation of this model can help Seattle Department of Transportation to classify the severity of accident more accurately and automatically with the data from the accident record. The model can be adjusted to include new variables if necessary.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.2 32-bit",
   "display_name": "Python 3.8.2 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "6fa8c4a0213b3e8e46e64ca221d4ef2f7254b1e53b83d6209b624a99d7aa7db4"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}